{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Solve the issue by using langchain and start a pull request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from autocoder.models import Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_url = \"https://github.com/shroominic/VoiceGPT/issues/5\"\n",
    "repo_url, issue_number = issue_url.split(\"/issues/\")\n",
    "\n",
    "repo = Repository(repo_url, getenv(\"GITHUB_TOKEN\"))\n",
    "issue = repo.get_issue(issue_number)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init prompt process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(model=\"gpt-4\", verbose=True, request_timeout=60*5, )\n",
    "chatgpt3 = ChatOpenAI(model=\"gpt-3.5-turbo\", verbose=True, request_timeout=60*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = {\n",
    "    \"repo_name\": issue.repository.name, \n",
    "    \"repo_keywords\": issue.repository.keywords, \n",
    "    \"repo_description\": issue.repository.description, \n",
    "    \"issue_title\": issue.title, \n",
    "    \"issue_number\": issue.issue_number,\n",
    "    \"issue_description\": issue.body, \n",
    "    \"code_tree\": issue.repository.codebase.tree\n",
    "}\n",
    "\n",
    "coding_system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    template=\n",
    "        \"You are a coding assistant solving tasks for developers.\"\n",
    "        \"Tasks are represented as issues on GitHub repositories.\"\n",
    "        \"Be verbose and precise in your responses.\\n\"\n",
    "        \"The following is a conversation about solving issue {issue_title} #{issue_number} on {repo_name}.\\n\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect important files to gain context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_files = HumanMessagePromptTemplate.from_template(template=\n",
    "\"\"\"\n",
    "Which files are important to read for gaining understanding of the codebase?\\n\n",
    "Codebase: \\n{code_tree}\\n\n",
    "For example in a python project you might want to look at the 'requirements.txt' file to understand which technologies are used in the project.\\n\n",
    "# Reply like this and make sure pathes are in 'single quotes' and not \"double quotes\":\n",
    "relevant_files = [\n",
    "    'path/to/file1.txt',  # do not start with ./ or /\n",
    "    'path/to/file2.js',\n",
    "    ...\n",
    "    # max 4 files so choose wisely\n",
    "]\n",
    "\"\"\")\n",
    "\n",
    "get_important_files = ChatPromptTemplate.from_messages([coding_system_prompt, important_files])\n",
    "\n",
    "files_chain = LLMChain(llm=chatgpt, prompt=get_important_files, verbose=True)\n",
    "files_ok = False\n",
    "tries = 0\n",
    "while not files_ok:\n",
    "    relevant_files_response = files_chain.run(info_dict)\n",
    "    if relevant_files_response:\n",
    "        print(relevant_files_response)\n",
    "        relevant_file_paths = re.findall(r\"'.*?'\", relevant_files_response)\n",
    "        files_ok = issue.codebase.validate_file_paths(relevant_file_paths)\n",
    "    else: \n",
    "        print(\"No files found try again\", relevant_files_response)\n",
    "        tries += 1\n",
    "    if tries > 5: raise Exception(\"No files found\")\n",
    "    \n",
    "print(\"Relevant Files:\", relevant_file_paths)\n",
    "\n",
    "relevant_files_dict = {}\n",
    "for file in relevant_file_paths:\n",
    "        file_content = issue.codebase.show_file(file.strip(\"'\"))\n",
    "        relevant_files_dict[file] = file_content\n",
    "\n",
    "info_dict[\"relevant_files\"] = str(relevant_files_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate repository summary/context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_summary = HumanMessagePromptTemplate.from_template(\n",
    "    template=\n",
    "        \"Summarize what this repository is about and what it does.\\n\"\n",
    "        \"Repository name: {repo_name}\\n\"\n",
    "        \"Repository description: {repo_description}\\n\"\n",
    "        \"Repository keywords: {repo_keywords}\\n\"\n",
    "        \"Repository codebase: \\n{code_tree}\\n\"\n",
    "        \"Relevant files: \\n{relevant_files}\\n\"\n",
    "        \"Describe the technologies used and the structure of the codebase. \"\n",
    "        \"Please be as detailed and precise as possible but keep it short.\\n\"\n",
    "    )\n",
    "get_repo_summary = ChatPromptTemplate.from_messages([coding_system_prompt, repo_summary])\n",
    "repo_summary = LLMChain(llm=chatgpt, prompt=get_repo_summary, verbose=True).run(info_dict)\n",
    "info_dict[\"repo_summary\"] = repo_summary\n",
    "repo_summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate issue summary/context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_summary = HumanMessagePromptTemplate.from_template(\n",
    "    template=\n",
    "        \"Repository Summary: \\n{repo_summary}\\n\"\n",
    "        \"Issue title: {issue_title}\\n\"\n",
    "        \"Issue description: {issue_description}\\n\"\n",
    "        \"Describe step by step (abstract) how to implement the issue and what files are relevant. \"\n",
    "        \"Be precise and keep it short.\\n\"\n",
    "    )\n",
    "get_issue_summary = ChatPromptTemplate.from_messages([coding_system_prompt, issue_summary])\n",
    "issue_summary = LLMChain(llm=chatgpt, prompt=get_issue_summary, verbose=True).run(info_dict)\n",
    "info_dict[\"issue_summary\"] = issue_summary\n",
    "issue_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide which files to change and create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [\"'src/web/main.py'\", \"'src/web/templates/pricing.html'\"]\n",
    "issue.codebase.validate_file_paths(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_changes = HumanMessagePromptTemplate.from_template(\n",
    "    template=\n",
    "    \"Codebase:\\n{code_tree}\\n\"\n",
    "    \"Repository description: {repo_summary}\\n\"\n",
    "    \"Issue description: {issue_summary}\\n\"\n",
    "    \"What files need to be changed to solve the issue?\\n\"\n",
    "    \"What new files need to be created?\\n\"\n",
    "    \n",
    "    \"Reply like this:\\n\"\n",
    "    \"files_to_change = [\\n\"\n",
    "    \"    'path/to/file1.txt',  # do not start with ./ or /\\n\"\n",
    "    \"    'path/to/file2.js',\\n\"\n",
    "    \"    ...\\n\"\n",
    "    \"]\\n\"\n",
    "    \"new_files = [\\n\"\n",
    "    \"    'path/to/new_file1.py',\\n\"\n",
    "    \"    ...\\n\"\n",
    "    \"]\\n\"\n",
    "    \"# write paths using 'single quotes', not \\\"double quotes\\\" and not `backticks`\\n\"\n",
    "    \"# make sure to put only file paths that exist in the codebase in files_to_change\\n\"\n",
    ")\n",
    "get_files_to_change = ChatPromptTemplate.from_messages([coding_system_prompt, prepare_changes])\n",
    "\n",
    "what_files_to_change = LLMChain(llm=chatgpt, prompt=get_files_to_change, verbose=True)\n",
    "files_ok = False\n",
    "tries = 0\n",
    "while not files_ok:\n",
    "    changes = what_files_to_change.run(info_dict)\n",
    "    print(\"OUTPUT:\", changes)\n",
    "    files_to_change = re.search(r\"files_to_change = \\[(.*?)\\]\", changes, re.DOTALL)\n",
    "    new_files = re.search(r\"new_files = \\[(.*?)\\]\", changes, re.DOTALL)\n",
    "    print(files_to_change.group(1) if files_to_change else \"No files to change\")\n",
    "    print(new_files.group(1) if new_files else \"No new files\")\n",
    "    if new_files:\n",
    "        new_file_paths = re.findall(r\"\\'(.*?)\\'\", new_files.group(1))\n",
    "        change_file_paths = re.findall(r\"\\'(.*?)\\'\", files_to_change.group(1))\n",
    "        files_ok = issue.codebase.validate_file_paths(change_file_paths)\n",
    "        print(\"Validadte:\", change_file_paths)\n",
    "        print(\"Files OK: \", files_ok)\n",
    "    elif files_to_change:\n",
    "        change_file_paths = re.findall(r\"\\'(.*?)\\'\", files_to_change.group(1))\n",
    "        files_ok = issue.codebase.validate_file_paths(change_file_paths)\n",
    "        print(\"No files found try again\", files_to_change)\n",
    "        tries += 1\n",
    "    if tries > 5: raise Exception(\"No files found\")\n",
    "\n",
    "print(\"Files to change:\\n\", change_file_paths, \"\\n\")\n",
    "print(\"New Files:\\n\", new_file_paths, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = ChatPromptTemplate.from_messages([\n",
    "    coding_system_prompt, \n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        template=\n",
    "            \"Repository description: {repo_summary}\\n\"\n",
    "            \"Issue description: {issue_summary}\\n\"\n",
    "            \"Implement this file {file_path} to solve the issue.\\n\"\n",
    "            \"After completion this the file will be created and added to the codebase.\\n\"\n",
    "            \"Reply with a codeblock containing the content of the new file.\\n\")])\n",
    "\n",
    "new_files_dict = {}\n",
    "for file_path in new_file_paths:\n",
    "    info_dict[\"file_path\"] = file_path\n",
    "    new_file_content = LLMChain(llm=chatgpt, prompt=new_file, verbose=True).run(info_dict)\n",
    "    print(new_file_content)\n",
    "    \n",
    "    # Extract the codeblock\n",
    "    new_content = re.search(r'```python(.*?)```', new_file_content, re.DOTALL).group(1)\n",
    "    \n",
    "    new_files_dict[file_path] = new_content\n",
    "\n",
    "\n",
    "\n",
    "change_file = ChatPromptTemplate.from_messages([\n",
    "    coding_system_prompt, \n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        template=\n",
    "\"\"\"\n",
    "Repository description:\n",
    "{repo_summary}\n",
    "Issue description:\n",
    "{issue_summary}\n",
    "First write a detailed instruction on how to change the file {file_path}.\n",
    "\n",
    "# [Begin file content]\n",
    "{file_content}\n",
    "# [Eind file content]\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "Then create a list called \"changes\".\n",
    "This list should contain tuples of the form (line_number, action, \"new code\").\n",
    "Action can be one of the following:\n",
    "- add: insert a new line of code before the line with the given number\n",
    "- overwrite: overwrite the line with the given number\n",
    "- delete: delete the line with the given number\n",
    "\n",
    "Write down all changes and \n",
    "reply with a codeblock like this:\n",
    "```python\n",
    "# (line_number, 'action', 'new code')\n",
    "# make sure use 'single quotes' and not \"double quotes\" or `backticks` like in this example:\n",
    "changes = [\n",
    "    (0, 'add', 'import newexample'),\n",
    "    (4, 'overwrite', 'def foo():'),\n",
    "    (5, 'add', '    print(\"hello world\")'),\n",
    "    (6, 'delete', '    print(\"bar\")'),\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\"\"\"\n",
    ")])\n",
    "\n",
    "change_files_dict = {}\n",
    "for file_path in change_file_paths:\n",
    "    file_content = issue.codebase.show_file(file_path.strip(\"'\"))\n",
    "    info_dict[\"file_path\"] = file_path\n",
    "    info_dict[\"file_content\"] = file_content\n",
    "    change_instructions = LLMChain(llm=chatgpt, prompt=change_file, verbose=True).run(info_dict)\n",
    "    print(change_instructions)\n",
    "    \n",
    "    # Extract the codeblock\n",
    "    changes_content = re.search(r'```python(.*?)```', change_instructions, re.DOTALL).group(1)\n",
    "\n",
    "    # Match each tuple inside the changes list\n",
    "    tuple_pattern = re.compile(r\"\\((\\d+),\\s*\\'(.*?)\\',\\s*\\'(.*?)\\'\\)\", re.DOTALL)\n",
    "    tuples = tuple_pattern.findall(changes_content)\n",
    "\n",
    "    # Create a list of tuples representing the same content\n",
    "    result = [(int(line), action, change) for line, action, change in tuples]\n",
    "\n",
    "    # for line in result: print(line)\n",
    "    \n",
    "    change_files_dict[file_path] = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, content in new_files_dict.items():\n",
    "    issue.codebase.create_file(file, content)\n",
    "    print(\"Created:\", file, \"\\nwith content:\", content, \"\\n\")\n",
    "    \n",
    "for file, changes in change_files_dict.items():\n",
    "    issue.codebase.change_file(file, changes)\n",
    "    print(\"Changed:\", file, \"\\nwith changes:\", changes, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data = \"\"\"\n",
    "Here are the changes as a list of tuples with the form `(line_number, 'action', 'new code')`:\n",
    "```python\n",
    "changes = [\n",
    "    # (line_number, 'action', 'new code')\n",
    "    (5, 'add', 'fastapi-users[oauth]==10.0.0'),\n",
    "    (6, 'add', 'httpx==0.22.0'),\n",
    "]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Extract the codeblock\n",
    "changes_pattern = re.compile(r'```python(.*?)```', re.DOTALL)\n",
    "changes_content = changes_pattern.search(data).group(1)\n",
    "\n",
    "# Match each tuple inside the changes list\n",
    "tuple_pattern = re.compile(r\"\\((\\d+),\\s*\\'(.*?)\\',\\s*\\'(.*?)\\'\\)\", re.DOTALL)\n",
    "tuples = tuple_pattern.findall(changes_content)\n",
    "\n",
    "# Create a list of tuples representing the same content\n",
    "result = [(int(line), action, change) for line, action, change in tuples]\n",
    "\n",
    "for line in result: print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test the code and repeat until it works\n",
    "\n",
    "# TODO: create a pull request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autocoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
